package com.farhannz.kaitou.helpers

//import ai.djl.nn.core.Embedding
import com.farhannz.kaitou.data.models.TokenInfo
import com.farhannz.kaitou.data.models.WordFull
import kotlinx.serialization.json.Json
import kotlin.math.exp
import kotlin.math.max
import kotlin.math.sqrt

object TokenHelper {
    private val LOG_TAG = TokenHelper::class.simpleName
    private val logger = Logger(LOG_TAG!!)
    private val json = Json { ignoreUnknownKeys = true }

    val coordinatingParticles = mapOf(
        "ã¨" to "and / with (coordinating particle)",
        "ã‚„" to "and / such as (listing)",
        "ã¨ã‹" to "and / or (informal listing)",
        "ã‚‚" to "also / too",
        "ã—" to "and / moreover"
    )

    val sentenceFinalParticles = mapOf(
        "ã‹" to "question marker",
        "ã­" to "seeking agreement / softener",
        "ã‚ˆ" to "assertion / emphasis",
        "ãª" to "prohibition / exclamation (masc.)",
        "ã" to "strong assertion (masc.)",
        "ã•" to "casual emphasis",
        "ã‚" to "soft feminine tone",
        "ã®" to "soft assertion / explanation",
        "ãœ" to "masculine emphasis / encouragement",
        "ã‹ãª" to "self-questioning / uncertainty",
        "ã‹ã—ã‚‰" to "I wonderâ€¦ (feminine)"
    )

    fun cosineSimilarity(a: FloatArray, b: FloatArray): Float {
        require(a.size == b.size)
        var dot = 0.0
        var normA = 0.0
        var normB = 0.0
        for (i in a.indices) {
            dot += a[i] * b[i]
            normA += a[i] * a[i]
            normB += b[i] * b[i]
        }
        val denom = sqrt(normA) * sqrt(normB)
        return if (denom == 0.0) Float.NEGATIVE_INFINITY else (dot / denom).toFloat()
    }

    data class RankedSense(
        val wordId: String,
        val senseId: Int,
        val score: Float,
        val kanjiTexts: List<String>,
        val kanaTexts: List<String>,
        val glossTexts: List<String>,
        val partOfSpeech: List<String>,
    )

    data class RankedContextBag(
        val ranked: RankedSense,
        val context: String,
        val gloss: String
    )

    // Utility to L2â€normalize your float arrays
    fun normalize(vec: FloatArray): FloatArray {
        val norm = sqrt(vec.map { it * it }.sum())
        return if (norm > 0f) vec.map { it / norm }.toFloatArray() else vec
    }

    fun flat2DArray(input: Array<LongArray>): LongArray {
        val batchSize = input.size
        val seqLen = input[0].size
        val flattened = LongArray(batchSize * seqLen)
        for (i in input.indices) {
            System.arraycopy(input[i], 0, flattened, i * seqLen, seqLen)
        }

        return flattened
    }

    fun padSequence(input: LongArray, maxLen: Int): LongArray {
        val padded = input + LongArray(maxLen - input.size) { 0L }
        return padded
    }

    fun sigmoid(logits: Float): Float {
        return 1.0f / (1.0f + exp(-logits))
    }

    fun softmax(logits: List<Float>): FloatArray {
        val maxLogit = logits.maxOrNull() ?: 0f
        val expLogits = FloatArray(logits.size) { exp(logits[it] - maxLogit) }
        val sumExp = expLogits.sum()
        return FloatArray(logits.size) { expLogits[it] / sumExp }
    }

    // This function can be a top-level function or a member of TokenHelper, etc.
    fun extractLocalContextPhrase(
        currentTokenIndex: Int,
        allTokens: List<TokenInfo>,
        k: Int = 3
    ): String? {
        if (allTokens.isEmpty()) return null
        val startIndex = maxOf(0, currentTokenIndex - k)
        val endIndex = minOf(allTokens.size - 1, currentTokenIndex + k)

        val clauseStart = findClauseStart(currentTokenIndex, allTokens, startIndex)
        val clauseEnd = findClauseEnd(currentTokenIndex, allTokens, endIndex)

        // Ensure valid boundaries
        if (clauseStart > clauseEnd) {
            // Fallback to original window if clause detection fails
            val localTokens = allTokens.subList(startIndex, endIndex + 1)
            return localTokens.joinToString("") { it.surface }
        }

        val localTokens = allTokens.subList(clauseStart, clauseEnd + 1)
//        logger.DEBUG(localTokens.toString())
        return localTokens.joinToString("") { it.surface }
    }


    fun patchPotentialGodan(baseForm: String, inflectionType: String): String {
        val godanPotentialSuffixes =
            listOf("ã‘ã‚‹", "ã’ã‚‹", "ã¦ã‚‹", "ã§ã‚‹", "ã¹ã‚‹", "ã‚ã‚‹", "ã‚Œã‚‹", "ã›ã‚‹")

        val isPotential = inflectionType == "ä¸€æ®µ" &&
                godanPotentialSuffixes.any { baseForm.endsWith(it) }
        // Kuromoji marks potential forms of godan -su verbs as ä¸€æ®µ verbs ending in "ã›ã‚‹"
        val patchedBase = if (isPotential) {
            // Replace potential ending with godan dictionary ending
            when {
                baseForm.endsWith("ã›ã‚‹") -> baseForm.dropLast(2) + "ã™"
                baseForm.endsWith("ã‘ã‚‹") -> baseForm.dropLast(2) + "ã"
                baseForm.endsWith("ã’ã‚‹") -> baseForm.dropLast(2) + "ã"
                baseForm.endsWith("ã¦ã‚‹") -> baseForm.dropLast(2) + "ã¤"
                baseForm.endsWith("ã§ã‚‹") -> baseForm.dropLast(2) + "ã¥"
                baseForm.endsWith("ã¹ã‚‹") -> baseForm.dropLast(2) + "ã¶"
                baseForm.endsWith("ã‚ã‚‹") -> baseForm.dropLast(2) + "ã‚€"
                baseForm.endsWith("ã‚Œã‚‹") -> baseForm.dropLast(2) + "ã‚‹"
                else -> baseForm
            }
        } else {
            baseForm
        }
        return patchedBase
    }

    fun getBaseReadingFromInflected(token: TokenInfo): String {
        val inflected = katakanaToHiragana(token.reading)

        return when (token.inflectionType) {
            "ä¸€æ®µ" -> inflected + "ã‚‹"
            "äº”æ®µãƒ»ãƒ©è¡Œ" -> when (token.inflectionForm) {
                "æœªç„¶å½¢", "é€£ç”¨å½¢" -> inflected.dropLast(1) + "ã‚‹"
                else -> inflected
            }

            "äº”æ®µãƒ»ã‚«è¡Œ" -> when (token.inflectionForm) {
                "æœªç„¶å½¢" -> inflected.dropLast(1) + "ã"
                "é€£ç”¨å½¢" -> inflected.dropLast(1) + "ã"
                else -> inflected
            }

            "äº”æ®µãƒ»ã‚¬è¡Œ" -> when (token.inflectionForm) {
                "æœªç„¶å½¢" -> inflected.dropLast(1) + "ã"
                "é€£ç”¨å½¢" -> inflected.dropLast(1) + "ã"
                else -> inflected
            }

            "äº”æ®µãƒ»ã‚«è¡Œã‚¤éŸ³ä¾¿" -> when (token.inflectionForm) {
                "é€£ç”¨å½¢" -> inflected.dropLast(1) + "ã"  // ãã„ â†’ ãã
                "æœªç„¶å½¢" -> inflected.dropLast(1) + "ã"  // ãã‹ â†’ ãã
                else -> inflected
            }

            "äº”æ®µãƒ»ã‚µè¡Œ" -> when (token.inflectionForm) {
                "æœªç„¶å½¢" -> inflected.dropLast(1) + "ã™"
                "é€£ç”¨å½¢" -> inflected.dropLast(1) + "ã™"
                else -> inflected
            }

            "äº”æ®µãƒ»ã‚¿è¡Œ" -> when (token.inflectionForm) {
                "æœªç„¶å½¢" -> inflected.dropLast(1) + "ã¤"
                "é€£ç”¨å½¢" -> inflected.dropLast(1) + "ã¤"
                else -> inflected
            }

            "äº”æ®µãƒ»ãƒŠè¡Œ" -> when (token.inflectionForm) {
                "æœªç„¶å½¢" -> inflected.dropLast(1) + "ã¬"
                "é€£ç”¨å½¢" -> inflected.dropLast(1) + "ã¬"
                else -> inflected
            }

            "äº”æ®µãƒ»ãƒè¡Œ" -> when (token.inflectionForm) {
                "æœªç„¶å½¢" -> inflected.dropLast(1) + "ã¶"
                "é€£ç”¨å½¢" -> inflected.dropLast(1) + "ã¶"
                else -> inflected
            }

            "äº”æ®µãƒ»ãƒè¡Œ" -> when (token.inflectionForm) {
                "æœªç„¶å½¢" -> inflected.dropLast(1) + "ã‚€"
                "é€£ç”¨å½¢" -> inflected.dropLast(1) + "ã‚€"
                else -> inflected
            }

            "äº”æ®µãƒ»ãƒ¯è¡Œ" -> when (token.inflectionForm) {
                "æœªç„¶å½¢" -> inflected.dropLast(1) + "ã†"
                "é€£ç”¨å½¢" -> inflected.dropLast(1) + "ã†"
                else -> inflected
            }

            else -> inflected
        }
    }

    private fun findClauseStart(currentIndex: Int, tokens: List<TokenInfo>, minIndex: Int): Int {
        for (i in currentIndex downTo minIndex) {
            val token = tokens[i]
            if (token.partOfSpeech.startsWith("åŠ©è©") || token.surface in listOf(
                    "ã€",
                    "ã€‚",
                    "ãŒ",
                    "ã¯"
                )
            ) {
                return maxOf(minIndex, minOf(i + 1, currentIndex)) // Ensure start â‰¤ currentIndex
            }
        }
        return minIndex
    }

    private fun findClauseEnd(currentIndex: Int, tokens: List<TokenInfo>, maxIndex: Int): Int {
        for (i in currentIndex..maxIndex) {
            val token = tokens[i]
            if (token.partOfSpeech.startsWith("åŠ©è©") || token.surface in listOf(
                    "ã€",
                    "ã€‚",
                    "ãŒ",
                    "ã¯",
                    "ã‚’",
                    "ã«"
                )
            ) {
                return minOf(maxIndex, maxOf(i, currentIndex)) // Ensure end â‰¥ currentIndex
            }
        }
        return maxIndex
    }

    fun rerankGlosses(
        entries: List<WordFull>,
        localContext: String,
        topN: Int = 3
    ): List<RankedSense> {
        val rankedContextBags = mutableListOf<RankedContextBag>()
        entries.forEach { word ->
            val kanjiMap = word.kanji.associateBy { it.kanjiId }
            val kanaMap = word.kana.associateBy { it.kanaId }
            val relevantKanji = mutableListOf<String>()
            val relevantKana = mutableListOf<String>()
            val senses = word.senses
            for (senseWithGloss in senses) {
                val sense = senseWithGloss.sense
                val glosses = senseWithGloss.glosses.map { it.text }
                val examples = senseWithGloss.examples
                val exampleTextBySenseId: Map<Int, String> = examples.associateBy(
                    { it.example.senseId },
                    {
                        it.sentences.firstOrNull { sentence -> sentence.language == "jpn" }?.text
                            ?: ""
                    }
                )
                try {
                    // Determine which kanji/kana apply to this sense
                    val kanji = if (sense.appliesToKanji.contains("*")) {
                        word.kanji.map { it.text }
                    } else {
                        sense.appliesToKanji.mapNotNull { id ->
                            kanjiMap.getValue(id.code).text
                        }
                    }
                    relevantKanji.addAll(kanji)

                    val kana = if (sense.appliesToKana.contains("*")) {
                        word.kana.map { it.text }
                    } else {
                        sense.appliesToKana.mapNotNull { id ->
                            kanaMap.getValue(id.code).text
                        }
                    }
                    relevantKana.addAll(kana)
                } catch (e: Throwable) {
                    logger.ERROR(e.toString())
                }
                val glossText = glosses.take(3).joinToString(", ")
                val example = exampleTextBySenseId[sense.senseId]?.let {
                    "e.g., $it"
                } ?: ""
                val combinedText = "$glossText $example"
                logger.DEBUG(combinedText)
                val placeholderScore = 1337f
                val ranked = RankedSense(
                    wordId = word.word.id,
                    senseId = sense.senseId!!,
                    score = placeholderScore,
                    kanjiTexts = relevantKanji,
                    kanaTexts = relevantKana,
                    glossTexts = glosses,
                    partOfSpeech = json.decodeFromString<List<String>>(sense.partOfSpeech)
                )
                rankedContextBags.add(RankedContextBag(ranked, localContext, glossText))
            }
        }
        var maxLen = Int.MIN_VALUE
        val inputIdsList = mutableListOf<LongArray>()
        val attentionMaskList = mutableListOf<LongArray>()
        rankedContextBags.forEach { (ranked, context, gloss) ->
            logger.DEBUG(gloss)
            val encoded = TransformerManager.tokenizePair(context, gloss)
            maxLen = max(encoded.ids.size, maxLen)
            inputIdsList.add(encoded.ids)
            attentionMaskList.add(encoded.attentionMask)
        }

        inputIdsList.indices.forEach { idx ->
            inputIdsList[idx] = padSequence(inputIdsList[idx], maxLen)
        }
        attentionMaskList.indices.forEach { idx ->
            attentionMaskList[idx] = padSequence(attentionMaskList[idx], maxLen)
        }
        println("${inputIdsList.size}, ${inputIdsList[0].size}")
        val flattenedIds = flat2DArray(inputIdsList.toTypedArray())
        val flattenedMasks = flat2DArray(attentionMaskList.toTypedArray())
        val result =
            TransformerManager.rankPairs(flattenedIds, flattenedMasks, inputIdsList.size.toLong())

        val contextLogitPair =
            rankedContextBags.zip(result.toList()).sortedByDescending { it.second }

        val topNScores = contextLogitPair.map { it.second }
        val activated = softmax(topNScores)
        val ranked = mutableListOf<RankedSense>()
        logger.DEBUG("Activated : ${activated.size} - rankedContextBag : ${rankedContextBags.size}")
        rankedContextBags.forEach { it ->
            logger.DEBUG("${it.context} - ${it.gloss}")
        }
        activated.forEachIndexed { index, score ->
            val temp = contextLogitPair[index].first.ranked
            ranked.add(
                RankedSense(
                    wordId = temp.wordId,
                    senseId = temp.senseId,
                    score = score,
                    kanjiTexts = temp.kanjiTexts,
                    kanaTexts = temp.kanaTexts,
                    glossTexts = temp.glossTexts,
                    partOfSpeech = temp.partOfSpeech
                )
            )
        }
        val output = ranked.sortedByDescending { it.score }
        output.forEach { logger.DEBUG("${it.glossTexts} ${it.kanjiTexts} ${it.kanaTexts} ${it.score} ${it.partOfSpeech}") }
        return output
    }

    fun rankDictionaryEntries(
        entries: List<WordFull>,
        localContextEmbedding: FloatArray?,
        sentenceEmbedding: FloatArray,
        mappedPOS: List<String>
    ): List<RankedSense> {
        if (entries.isEmpty()) {
            println("No dictionary entries found")
            return emptyList()
        }

        println("Found ${entries.size} dictionary entries:")
        // 1. Filter entries by POS match
        val posMatched = entries.filter { wordFull ->
            wordFull.senses.any { (sense, glosses) ->
                val pos = json.decodeFromString<List<String>>(sense.partOfSpeech)
                pos.any { it in mappedPOS }
            }
        }

        println("ALL ENTRIES DICT LOOKUP RESULT")
        entries.forEach {
            println(it)
        }
        // 2. Deduplicate by wordId
//        val unique = posMatched.distinctBy { it.word.id }
        val results = mutableListOf<RankedSense>()
        for (word in posMatched) {
            val kanjiMap = word.kanji.associateBy { it.kanjiId }
            val kanaMap = word.kana.associateBy { it.kanaId }
            val relevantKanji = mutableListOf<String>()
            val relevantKana = mutableListOf<String>()
            for (senseWithGloss in word.senses) {
                val sense = senseWithGloss.sense
                val glosses = senseWithGloss.glosses.map { it.text }
                val examples = senseWithGloss.examples
                logger.DEBUG(examples.joinToString(","))
                try {
                    // Determine which kanji/kana apply to this sense
                    val kanji = if (sense.appliesToKanji.contains("*")) {
                        word.kanji.map { it.text }
                    } else {
                        sense.appliesToKanji.mapNotNull { id ->
                            kanjiMap.getValue(id.code).text
                        }
                    }
                    relevantKanji.addAll(kanji)

                    val kana = if (sense.appliesToKana.contains("*")) {
                        word.kana.map { it.text }
                    } else {
                        sense.appliesToKana.mapNotNull { id ->
                            kanaMap.getValue(id.code).text
                        }
                    }
                    relevantKana.addAll(kana)
                } catch (e: Throwable) {
                    logger.ERROR(e.toString())
                }

                val glossText = glosses.joinToString(", ")

// Check if info is available and not empty
                val infoTexts = json.decodeFromString<List<String>>(sense.info)
                val infoText = if (infoTexts.isNotEmpty()) {
                    " (${infoTexts.joinToString(",")})"
                } else {
                    ""
                }

                val contextualGloss = glossText + infoText
                val canonicalGloss = glossText.substringBefore("(").trim()
//                logger.DEBUG("Contextual gloss : $contextualGloss")
                val contextualEmbedding = TransformerManager.getEmbeddings(canonicalGloss)
                logger.DEBUG("Contextual embedding size ${contextualEmbedding.size}")
                val score = cosineSimilarity(sentenceEmbedding, contextualEmbedding)
                var finalScore = score
//                -0.37447652
//                -0.048565257
//                0.7874593
//                0.11824332
                if (localContextEmbedding != null) {
                    val localScore = cosineSimilarity(localContextEmbedding, contextualEmbedding)
                    finalScore = 0.3f * score + 0.7f * localScore
                }
                logger.DEBUG("contextualGloss $contextualGloss")
//                logger.DEBUG("contextualGloss embedding: ${contextualEmbedding.joinToString(",")}")
                logger.DEBUG("PER SENSE SCORE $finalScore")
                results.add(
                    RankedSense(
                        wordId = word.word.id,
                        senseId = sense.senseId!!,
                        score = finalScore,
                        kanjiTexts = relevantKanji,
                        kanaTexts = relevantKana,
                        glossTexts = glosses,
                        partOfSpeech = json.decodeFromString<List<String>>(sense.partOfSpeech)
                    )
                )
            }
        }
        val sorted = results.sortedByDescending { it.score }
        logger.DEBUG(sorted.joinToString(","))
// Sort by score descending and take top 3 (or more if needed)
//        val top = scored.sortedByDescending { it.third }.take(3)
//        logger.DEBUG(top.toString())
//        val bestSenses = entries.map { entry ->
//            entry.senses.map { (sense, glosses )->
//                val score = glosses.map {
//
//                }
//            }
//        }
//        val posMatched = entries
//            .filter { entry ->
//                val posList = entry.senses.map {
//                    json.decodeFromString<List<String>>(it.sense.partOfSpeech)
//                }.distinctBy { it }
//                posList.any { it.any { pos -> pos in mappedPOS } }
//            }
//
//        posMatched.forEach {
//            logger.DEBUG(it.toString())
//        }

//        val unique = posMatched.distinctBy { it["word_id"] }


//        val textToVec = mutableMapOf<String, Embedding>()
//        unique.mapNotNull { entry ->
//            entry["kanji_text"] as? String          // 1st choice
//                ?: entry["kana_text"] as? String    // 2nd choice
//                ?: entry["gloss_text"] as? String   // last resort
//        }.distinct().forEach { text ->
//            textToVec[text] = if (useServer) {
//                sendEmbedRequest(text)
//            } else {
//                val ids = tokenizer.encode(text).ids
//                val result = model.run(ModelInput(ids))
//                Embedding(result.output.toList())
//            }
//        }
//
////        // 3. Produce the scored list
//        val scored = posMatched.map { entry ->
//            val vec =
//            val score = cosineSimilarity(sentenceEmbedding.value, vec.value)
//            entry to score
//        }
////
//        return scored.take(3).map {
//            val surface = (it.first["kanji_text"] as String?) ?: it.first["kana_text"] as String
//            val reading = (it.first["kana_text"] as String)
//            val meaning = (it.first["gloss_text"] as String)
//            val pos = (it.first["part_of_speech"] as String)
//
//            Pair(
//                TokenInfo("", surface, pos, reading, "", ""),
//                "$meaning (cosine similarity: ${"%.2f".format(it.second)})"
//            )
//        }
        return sorted
    }

    fun mergeWithDictionary(
        tokens: List<TokenInfo>,
        dict: Set<String>?,
        maxGram: Int = 6
    ): List<TokenInfo> {
        // Add before merging
        val filteredTokens = tokens.map {
            if (it.surface in setOf("!", "?", "ï¼", "ï¼Ÿ", "ã€‚", "ã€")) {
                it.copy(partOfSpeech = "è¨˜å·") // Mark as symbol
            } else it
        }
        val result = mutableListOf<TokenInfo>()
        var i = 0
        while (i < filteredTokens.size) {
            var merged: TokenInfo? = null
            outer@ for (n in maxGram downTo 2) {
                if (i + n > filteredTokens.size) continue
                val span = filteredTokens.subList(i, i + n)
                val text = span.joinToString("") { it.surface } // Always use surface form

// Special case: Allow particle + auxiliary verb merging
                val allowMerge = span.all { token ->
                    when {
                        token.partOfSpeech.startsWith("åŠ©è©") -> true // Allow particles
                        token.partOfSpeech.startsWith("åŠ©å‹•è©") -> true
                        token.partOfSpeech.startsWith("åè©") -> true
                        token.partOfSpeech.startsWith("å‹•è©") -> true
                        else -> false
                    }
                }

                if (allowMerge && dict?.contains(text) ?: false) {
                    // Preserve original surface but use first POS
                    merged = TokenInfo(text, text, span.first().partOfSpeech)
                    i += n
                    break@outer
                }
            }
            result.add(merged ?: filteredTokens[i])
            i += if (merged != null) 0 else 1
        }
        return result
    }

    fun correctAuxiliaryNegative(tokens: List<TokenInfo>): List<TokenInfo> {
        return tokens.mapIndexed { i, token ->
            when {
                // Rule 1: Correct ãªã„ after particles
                token.surface == "ãªã„" && i > 0 && tokens[i - 1].surface == "ã¯" ->
                    token.copy(partOfSpeech = "åŠ©å‹•è©")

                // Rule 2: Correct ãªã„ after verbs
                token.surface == "ãªã„" && i > 0 && tokens[i - 1].partOfSpeech.startsWith("å‹•è©") ->
                    token.copy(partOfSpeech = "åŠ©å‹•è©")

                // Rule 3: Handle common negative forms
                token.surface in setOf("ã‚“", "ãš") ->
                    token.copy(partOfSpeech = "åŠ©å‹•è©")

                else -> token
            }
        }
    }

// Taken from https://is.muni.cz/th/opw9s/Thesis___Analyzer_of_Japanese_Texts_for_Language_Learning_Purposes.pdf
    /*
        Algorithm 1 Concatenation of N-grams
        procedure concatenateNgrams(ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘ )
            ğ‘– â† 0
            while ğ‘– < ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘ .ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„ do
                ğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’ğ‘‘ğ‘Šğ‘œğ‘Ÿğ‘‘ â† ğ‘šğ‘ğ‘¡ğ‘â„ğ‘Šğ‘œğ‘Ÿğ‘‘(ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘ , ğ‘–)
                ğ‘– â† ğ‘– + ğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’ğ‘‘ğ‘Šğ‘œğ‘Ÿğ‘‘.ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘š.ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„
                end while
            end procedure

        function matchWord(ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘ , ğ‘œğ‘“ ğ‘“ ğ‘ ğ‘’ğ‘¡)
            ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘šğ‘†ğ‘–ğ‘§ğ‘’ â† 4
            while ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘šğ‘†ğ‘–ğ‘§ğ‘’ > 0 do
                ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘š â† ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘ [ğ‘œğ‘“ ğ‘“ ğ‘ ğ‘’ğ‘¡ âˆ¶ ğ‘œğ‘“ ğ‘“ ğ‘ ğ‘’ğ‘¡ + ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘šğ‘†ğ‘–ğ‘§ğ‘’]
                â–· Get the surface form from all the n-gram tokens but the last and the dictionary form from the last token:
                ğ‘ğ‘Ÿğ‘’ğ‘“ ğ‘–ğ‘¥ğ‘†ğ‘¢ğ‘Ÿğ‘“ ğ‘ğ‘ğ‘’ğ‘  â† ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘š[0 âˆ¶ ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘šğ‘†ğ‘–ğ‘§ğ‘’ âˆ’ 1].ğ‘šğ‘ğ‘(ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘› â†’ ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›.ğ‘ ğ‘¢ğ‘Ÿğ‘“ ğ‘ğ‘ğ‘’).ğ‘—ğ‘œğ‘–ğ‘›()
                ğ‘¤ğ‘œğ‘Ÿğ‘‘ â† ğ‘ğ‘Ÿğ‘’ğ‘“ ğ‘–ğ‘¥ğ‘†ğ‘¢ğ‘Ÿğ‘“ ğ‘ğ‘ğ‘’ğ‘  + ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘š[ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘šğ‘†ğ‘–ğ‘§ğ‘’ âˆ’ 1].ğ‘‘ğ‘–ğ‘ğ‘¡
                if ğ‘¤ğ‘œğ‘Ÿğ‘‘ in ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ğ‘Ÿğ‘¦ then
                    return {ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘š âˆ¶ ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘š, ğ‘’ğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘  âˆ¶ ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ğ‘Ÿğ‘¦[ğ‘¤ğ‘œğ‘Ÿğ‘‘]}
                end if
                ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘šğ‘†ğ‘–ğ‘§ğ‘’ â† ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘šğ‘†ğ‘–ğ‘§ğ‘’ âˆ’ 1
            end while
            return {ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘š âˆ¶ [ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘ [ğ‘œğ‘“ ğ‘“ ğ‘ ğ‘’ğ‘¡]]}
        end function
    */
}